import asyncio
import json
import mysql.connector
import click
import time
import gc
from datetime import datetime, timedelta
from pathlib import Path
from itertools import chain

from fpds.cli.parts.utils import process_booleans, log_missing_keys
from fpds.cli.parts.contract_parser import extract_contract_data
from fpds.cli.parts.columns import columns
from fpds.cli.parts.bool_fields import bool_fields
from fpds import fpdsRequest
from fpds.config import DB_CONFIG

import warnings
warnings.filterwarnings("ignore")

BATCH_SIZE = 1000
DATA_DIR = Path("/Users/iliaoborin/fpds/data/")


def get_db_connection():
    try:
        return mysql.connector.connect(**DB_CONFIG)
    except mysql.connector.Error as e:
        click.echo(f"‚ö†Ô∏è Database connection error: {e}")
        return None


def get_last_parsed_date():
    conn = get_db_connection()
    if not conn:
        return None, None

    cursor = conn.cursor()
    cursor.execute(
        "SELECT parsed_date, status FROM parser_stage ORDER BY parsed_date DESC LIMIT 1")
    last_parsed_record = cursor.fetchone()
    conn.close()

    if last_parsed_record:
        return datetime.strptime(str(last_parsed_record[0]), "%Y-%m-%d"), last_parsed_record[1]
    return datetime(1957, 9, 30), 'completed'  # –ù–∞—á–∏–Ω–∞–µ–º —Å 1957-09-30


def check_existing_file(date):
    year, month, day = date.split("/")
    file_path = DATA_DIR / year / f"{month}_{day}.json"

    click.echo(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å: {file_path}")  # –õ–æ–≥–∏—Ä—É–µ–º –ø—É—Ç—å

    if not file_path.exists():
        click.echo("‚ùå –§–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")
        return None

    if file_path.stat().st_size == 0:
        click.echo("‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç.")
        return None

    try:
        with open(file_path, "r") as f:
            json.load(f)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ª–∏ JSON
        click.echo("‚úÖ –§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ.")
        return file_path
    except json.JSONDecodeError:
        click.echo("‚ùå –§–∞–π–ª –±–∏—Ç—ã–π (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON).")
        return None


def fetch_fpds_data(date):
    formatted_date = f"SIGNED_DATE=[{date},{date}]"
    params = dict([formatted_date.split("=")])
    request = fpdsRequest(**params, cli_run=True)
    print("üåê –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º FPDS –¥–∞–Ω–Ω—ã–µ...")

    data = asyncio.run(request.data())  # data - —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤
    return list(chain.from_iterable(data))  # –î–µ–ª–∞–µ–º –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫


def save_data_to_file(data, file_path):
    file_path.parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, "w") as outfile:
        json.dump(data, outfile, indent=4)  # –î–æ–±–∞–≤–∏–º –æ—Ç—Å—Ç—É–ø—ã –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    print(f"üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –≤ JSON: {file_path}")


def generate_file_path(date):
    """–°–æ–∑–¥–∞—ë—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É JSON –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç—ã."""
    year, month, day = date.split("/")
    return DATA_DIR / year / f"{month}_{day}.json"

def log_parsing_result(parsed_date, file_path, status, update=False):
    conn = get_db_connection()
    if not conn:
        return False

    cursor = conn.cursor()
    if update:
        cursor.execute(
            "UPDATE parser_stage SET status = %s, updated_at = NOW() WHERE parsed_date = %s",
            (status, parsed_date)
        )
    else:
        cursor.execute(
            "INSERT INTO parser_stage (parsed_date, file_path, status, created_at, updated_at) "
            "VALUES (%s, %s, %s, NOW(), NOW())",
            (parsed_date, file_path, status)
        )
    conn.commit()
    conn.close()
    return True


def insert_into_clickhouse(client, batch):
    asyncio.run(asyncio.to_thread(client.insert,
                "raw_contracts", batch, column_names=columns))


def process_data_and_insert(file_path, client):
    with open(file_path, "r") as f:
        records = json.load(f)

    total_inserted = 0
    for i in range(0, len(records), BATCH_SIZE):
        batch = []
        for contract in records[i:i + BATCH_SIZE]:
            partition_year = datetime.strptime(contract.get(
                "signed_date", "2000-01-01"), "%Y-%m-%d").year
            contract = process_booleans(contract, bool_fields)
            contract_data = extract_contract_data(contract, partition_year)
            log_missing_keys(contract, columns, file_path)
            batch.append(contract_data)

        if batch:
            insert_into_clickhouse(client, batch)
            total_inserted += len(batch)
            click.echo(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_inserted} –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –≤ ClickHouse")
            gc.collect()
            time.sleep(5)


@click.command()
@click.argument("date")
def parse_clickhouse(date):
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤—Å—Ç–∞–≤–∫–∞ –≤ ClickHouse."""

    last_parsed_date, last_status = get_last_parsed_date()

    # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
    click.echo(
        f"üìÖ –ü–æ—Å–ª–µ–¥–Ω—è—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–∞—Ç–∞: {last_parsed_date.strftime('%Y-%m-%d')}, –°—Ç–∞—Ç—É—Å: {last_status}")

    if last_status == "completed":
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –¥–∞—Ç—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'pending'
        next_parsing_date = last_parsed_date + timedelta(days=1)
        click.echo(
            f"üìù –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –¥–∞—Ç—É –≤ –ë–î: {next_parsing_date.strftime('%Y-%m-%d')} —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'pending'")
        file_path = generate_file_path(next_parsing_date.strftime('%Y/%m/%d'))
        log_parsing_result(next_parsing_date.strftime(
            '%Y-%m-%d'), str(file_path), "pending")

    elif last_status == "pending":
        # –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        file_path = generate_file_path(last_parsed_date.strftime('%Y/%m/%d'))
        click.echo(
            f"üåê –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {last_parsed_date.strftime('%Y-%m-%d')}")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ 'running'
        log_parsing_result(last_parsed_date.strftime(
            '%Y-%m-%d'), str(file_path), "running", update=True)

        data = fetch_fpds_data(last_parsed_date.strftime('%Y/%m/%d'))

        if not data:
            click.echo(
                f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ {last_parsed_date.strftime('%Y-%m-%d')}, —Å—Ç–∞–≤–∏–º 'completed'")
            log_parsing_result(last_parsed_date.strftime(
                '%Y-%m-%d'), str(file_path), "completed", update=True)
            return
        
        # ‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        file_path = generate_file_path(last_parsed_date.strftime('%Y/%m/%d'))

        save_data_to_file(data, file_path)
        click.echo(f"üìÑ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")

    elif last_status == "running":
        click.echo("üîÑ –ü–∞—Ä—Å–∏–Ω–≥ —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –¥–∞–Ω–Ω—ã–µ
        file_path = check_existing_file(last_parsed_date.strftime('%Y/%m/%d'))

        if file_path:
            click.echo(
                f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {file_path}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –≤—Å—Ç–∞–≤–∫—É –≤ ClickHouse")

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ 'completed'
            log_parsing_result(last_parsed_date.strftime(
                '%Y-%m-%d'), str(file_path), "completed", update=True)
            click.echo("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ ClickHouse, —Å—Ç–∞—Ç—É—Å 'completed'")

        else:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç–∞—Ç—É—Å–µ 'running', –º–µ–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ 'failed'.")
            log_parsing_result(last_parsed_date.strftime('%Y-%m-%d'), "", "failed", update=True)

    elif last_status == "failed":
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
        click.echo(
            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –ø–∞—Ä—Å–∏–Ω–≥–µ {last_parsed_date.strftime('%Y-%m-%d')}, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞.")
        file_path = check_existing_file(last_parsed_date.strftime('%Y/%m/%d'))

        if file_path:
            click.echo(
                f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {file_path}, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ ClickHouse")
        else:
            click.echo("‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ.")
            data = fetch_fpds_data(last_parsed_date.strftime('%Y/%m/%d'))

            if not data:
                click.echo(
                    "‚ùå –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ –¥–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Å—Ç–∞—ë—Ç—Å—è 'failed'.")
                return

            # ‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º 
            file_path = generate_file_path(last_parsed_date.strftime('%Y/%m/%d'))
            save_data_to_file(data, file_path)


        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ 'completed'
        log_parsing_result(last_parsed_date.strftime(
            '%Y-%m-%d'), str(file_path), "completed", update=True)
        click.echo("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ ClickHouse, —Å—Ç–∞—Ç—É—Å 'completed'")


@click.command(name="get")
@click.argument("date")
@click.option("-o", "--output", required=False, help="Output directory")
def get_fpds(date, output):
    """
    Download FPDS data for a specific date and save as JSON.

    Example:
        fpds get 2004/07/14
    """
    try:
        year, month, day = date.split("/")
    except ValueError:
        click.echo(
            f"‚ö†Ô∏è Invalid date format: {date}. Expected format: YYYY/MM/DD")
        return

    data_dir = Path(output) if output else Path("/Users/iliaoborin/fpds/data/")
    data_file = data_dir / year / f"{month}_{day}.json"

    formatted_date = f"SIGNED_DATE=[{date},{date}]"
    params = dict([formatted_date.split("=")])

    click.echo(f"üåê Downloading FPDS data for {date}...")

    request = fpdsRequest(**params, cli_run=True)

    try:
        data = asyncio.run(request.data())
        records = list(chain.from_iterable(data))

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        data_file.parent.mkdir(parents=True, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        with open(data_file, "w") as outfile:
            json.dump(records, outfile, indent=4)

        click.echo(f"‚úÖ Saved {len(records)} records to {data_file}")

        if not records:
            click.echo("‚ö†Ô∏è No records found. The file is empty.")

    except Exception as e:
        click.echo(f"‚ùå Error occurred: {e}")
