import json
import clickhouse_connect
import time
import mysql.connector
import pendulum
import sys
import ijson
from pathlib import Path
from datetime import datetime
from fpds.cli.parts.utils import process_booleans, log_missing_keys
from fpds.cli.parts.contract_parser import extract_contract_data
from fpds.cli.parts.columns import columns
from fpds.cli.parts.bool_fields import bool_fields
from fpds.config import DB_CONFIG

# üìå –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BATCH_SIZE = 1000
MAX_MEMORY_ERRORS = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø–∞–º—è—Ç–∏ –ø–æ–¥—Ä—è–¥

# ‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
print("üîÑ –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse...")
try:
    client = clickhouse_connect.get_client(
        host="localhost", port=8123, database="fpds_clickhouse"
    )
    print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ClickHouse: {e}")
    sys.exit(1)


def insert_batch_with_retry(client, table, batch, columns, file_id):
    wait_time = 10
    memory_error_count = 0
    batch_size = len(batch)

    while True:
        try:
            client.insert(table, batch, column_names=columns)
            memory_error_count = 0  # –£—Å–ø–µ—à–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ - —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫
            break
        except Exception as e:
            if "MEMORY_LIMIT_EXCEEDED" in str(e):
                memory_error_count += 1
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏ #{memory_error_count}: {e}")

                if memory_error_count >= MAX_MEMORY_ERRORS:
                    print(
                        "‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø–∞–º—è—Ç–∏. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å!")
                    update_status(file_id, "clickhouse_memory_failed")
                    sys.exit(10)  # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å —Å –∫–æ–¥–æ–º 10

                print(f"‚åõ –ñ–¥–µ–º {wait_time} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(wait_time)
                wait_time += 10

                if batch_size > 100:
                    batch_size -= 100

                batch = batch[:batch_size]
            else:
                raise


def get_db_connection():
    try:
        return mysql.connector.connect(**DB_CONFIG)
    except mysql.connector.Error as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
        return None


def get_next_file():
    conn = get_db_connection()
    if not conn:
        return None

    cursor = conn.cursor(dictionary=True)
    query = """
    SELECT id, signed_date, record_count, inserted_records, file_path, status
    FROM insert_json_clickhouse
    WHERE status = 'file_found'
    ORDER BY signed_date ASC
    LIMIT 1;
    """
    cursor.execute(query)
    file_data = cursor.fetchone()
    cursor.close()
    conn.close()

    if file_data:
        print(
            f"üìÇ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {file_data['file_path']} ({file_data['record_count']} –∑–∞–ø–∏—Å–µ–π)")
    return file_data


def update_status(file_id, status, inserted_records=0):
    conn = get_db_connection()
    if not conn:
        return

    cursor = conn.cursor()
    query = """
    UPDATE insert_json_clickhouse
    SET status = %s, inserted_records = %s, updated_at = NOW()
    WHERE id = %s;
    """
    cursor.execute(query, (status, inserted_records, file_id))
    conn.commit()
    cursor.close()
    conn.close()
    print(f"üìå –°—Ç–∞—Ç—É—Å –æ–±–Ω–æ–≤–ª–µ–Ω: {status}")


def process_data_and_insert(file_data):
    file_path = Path(file_data["file_path"])
    file_id = file_data["id"]
    expected_records = file_data["record_count"]
    inserted_records = file_data["inserted_records"]

    if inserted_records >= expected_records:
        print(f"‚úÖ –í—Å–µ –∑–∞–ø–∏—Å–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è {file_path}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        update_status(file_id, "clickhouse_loaded", inserted_records)
        return

    print(f"üìñ –û—Ç–∫—Ä—ã–≤–∞–µ–º JSON-—Ñ–∞–π–ª: {file_path}")

    total_inserted = inserted_records
    batch = []

    try:
        with open(file_path, "r") as f:
            parser = ijson.items(f, 'item')  # –ß–∏—Ç–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –º–∞—Å—Å–∏–≤–∞ JSON

            for idx, contract in enumerate(parser):
                if idx < inserted_records:
                    continue  # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —É–∂–µ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ

                contract = {k: v for k, v in contract.items(
                ) if k in columns or str(v).strip()}

                signed_date_keys = [
                    "content__award__relevantContractDates__signedDate",
                    "content__IDV__relevantContractDates__signedDate",
                    "content__OtherTransactionAward__contractDetail__relevantContractDates__signedDate",
                    "content__OtherTransactionIDV__contractDetail__relevantContractDates__signedDate",
                ]
                signed_date = next((contract.get(
                    k) for k in signed_date_keys if k in contract and contract[k]), None)
                if not signed_date:
                    raise ValueError(
                        f"‚ùå –û—à–∏–±–∫–∞! –í –∫–æ–Ω—Ç—Ä–∞–∫—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç `signed_date`. –ö–æ–Ω—Ç—Ä–∞–∫—Ç: {json.dumps(contract, indent=2)}")

                dt = pendulum.from_format(signed_date, "YYYY-MM-DD HH:mm:ss")
                contract = process_booleans(contract, bool_fields)
                contract_data = extract_contract_data(contract, dt.date())
                log_missing_keys(contract, columns, file_path)

                batch.append(contract_data)

                # –ï—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ BATCH_SIZE - –≤—Å—Ç–∞–≤–ª—è–µ–º
                if len(batch) >= BATCH_SIZE:
                    insert_batch_with_retry(
                        client, "raw_contracts", batch, columns, file_id)
                    total_inserted += len(batch)
                    print(
                        f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {total_inserted}/{expected_records} –∑–∞–ø–∏—Å–µ–π ({(total_inserted/expected_records)*100:.2f}%)")
                    time.sleep(2)
                    update_status(file_id, "clickhouse_loaded", total_inserted)
                    batch = []

            # –í—Å—Ç–∞–≤–∏—Ç—å –æ—Å—Ç–∞—Ç–∫–∏
            if batch:
                insert_batch_with_retry(
                    client, "raw_contracts", batch, columns, file_id)
                total_inserted += len(batch)
                update_status(file_id, "clickhouse_loaded", total_inserted)

        if total_inserted >= expected_records:
            update_status(file_id, "clickhouse_loaded", total_inserted)
            print("‚úÖ –§–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω!")
        else:
            update_status(file_id, "clickhouse_load_failed", total_inserted)
            print("‚ö†Ô∏è –ù–µ –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        update_status(file_id, "clickhouse_load_failed", total_inserted)


# üîÑ –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    file_data = get_next_file()

    if file_data:
        process_data_and_insert(file_data)
    else:
        print("üéâ –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        sys.exit(0)
