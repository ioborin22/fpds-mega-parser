import asyncio
import requests
import mysql.connector
import warnings
from datetime import datetime, timedelta
from lxml import etree
from urllib.parse import urlparse, parse_qs
from pathlib import Path
from fpds.config import DB_CONFIG

warnings.filterwarnings("ignore")

BASE_URL = "https://www.fpds.gov/ezsearch/FEEDS/ATOM?s=FPDS&FEEDNAME=PUBLIC&q=SIGNED_DATE:[{date},{date}]"

DATA_DIR = Path("/Users/iliaoborin/fpds/data/")
BATCH_SIZE = 1000


def get_db_connection():
    """ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ MySQL """
    try:
        return mysql.connector.connect(**DB_CONFIG)
    except mysql.connector.Error as e:
        print(f"‚ö†Ô∏è Database connection error: {e}")
        return None


def fetch_fpds_data(date):
    """ –ó–∞–≥—Ä—É–∂–∞–µ—Ç XML-–¥–∞–Ω–Ω—ã–µ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç–µ """
    url = BASE_URL.format(date=date)
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        return response.content
    except requests.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {date}: {e}")
        return None


def parse_pages(xml_data):
    """ –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ XML """
    try:
        tree = etree.fromstring(xml_data)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å—Ç—å –ª–∏ –∑–∞–ø–∏—Å–∏ –≤ XML (–∏—â–µ–º <entry>)
        entries = tree.findall(
            ".//atom:entry", namespaces={"atom": "http://www.w3.org/2005/Atom"})
        if not entries:
            return None  # –ï—Å–ª–∏ <entry> –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç

        last_link = tree.xpath(
            "//atom:link[@rel='last']/@href", namespaces={"atom": "http://www.w3.org/2005/Atom"})

        if not last_link:
            first_link = tree.xpath(
                "//atom:link[@rel='alternate']/@href", namespaces={"atom": "http://www.w3.org/2005/Atom"})
            if first_link and "start=0" in first_link[0]:
                return 10  # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ, –Ω–æ start=0, –∑–∞–ø–∏—Å—ã–≤–∞–µ–º 10
            return None  # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î

        parsed_url = urlparse(last_link[0])
        query_params = parse_qs(parsed_url.query)
        pages = int(query_params.get("start", [0])[0])
        return pages
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
    return None


def insert_into_db(date, pages):
    """ –í—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É signed_date_records """
    if pages is None:
        return  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∞—Ç—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö

    conn = get_db_connection()
    if not conn:
        return

    cursor = conn.cursor()
    updated_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    query = """
    INSERT INTO signed_date_records (signed_date, pages, updated_at)
    VALUES (%s, %s, %s)
    ON DUPLICATE KEY UPDATE pages=%s, updated_at=%s
    """
    cursor.execute(query, (date, pages, updated_at, pages, updated_at))
    conn.commit()
    cursor.close()
    conn.close()


def main():
    """ –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è """
    start_date = datetime(1957, 10, 1)
    today_date = datetime.now().date()
    current_date = start_date

    while current_date.date() <= today_date:
        date_str = current_date.strftime("%Y/%m/%d")
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç—ã: {date_str}")

        xml_data = fetch_fpds_data(date_str)
        if xml_data:
            pages = parse_pages(xml_data)
            if pages is not None:
                print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {pages}")
                insert_into_db(date_str, pages)

        current_date += timedelta(days=1)

    print("‚úÖ –ü–∞—Ä—Å–µ—Ä –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É. –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞.")


if __name__ == "__main__":
    main()
